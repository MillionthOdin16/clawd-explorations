#!/usr/bin/env python3
"""
Semantic Compression Techniques - How to Incorporate Semantics

This document outlines how semantic techniques can be integrated into
context compression for better information preservation.
"""

# =============================================================================
# 1. SEMANTIC CHUNKING (Better Than My Test)
# =============================================================================

def semantic_chunking(text, max_chunk_size=1000):
    """
    Split by semantic boundaries, not just newlines.

    Better approach:
    - Split at section headers (##)
    - Split at paragraph boundaries
    - Merge short related paragraphs
    - Split long paragraphs at logical points (not mid-sentence)
    """
    # My test just took highest-scoring paragraphs - BAD
    # Better: Preserve structure while scoring

    # Step 1: Identify semantic boundaries
    boundaries = []
    lines = text.split('\n')
    current_chunk = []

    for line in lines:
        # Header = new semantic unit
        if line.startswith('##'):
            if current_chunk:
                boundaries.append('\n'.join(current_chunk))
            current_chunk = [line]
        # Blank line = paragraph boundary
        elif not line.strip():
            if current_chunk:
                boundaries.append('\n'.join(current_chunk))
            current_chunk = []
        # Long line - split at sentence boundaries
        elif len(line) > max_chunk_size:
            sentences = re.split(r'(?<=[.!?])\s+', line)
            for sent in sentences:
                current_chunk.append(sent)
        else:
            current_chunk.append(line)

    if current_chunk:
        boundaries.append('\n'.join(current_chunk))

    return boundaries

# =============================================================================
# 2. SEMANTIC IMPORTANCE SCORING (With Embeddings)
# =============================================================================

def semantic_importance_scoring(chunks, critical_keywords):
    """
    Score chunks by semantic importance.

    Using embeddings (simulated without API):
    - Compare chunk to critical keywords (semantic similarity)
    - Score based on:
      * Contains critical information
      * Is foundational for understanding
      * Contains examples/illustrations
    """
    import numpy as np

    # Critical keywords (from research - what matters most)
    critical_terms = [
        'command', 'tool', 'file', 'path', 'rule', 'critical',
        'never', 'always', 'must', 'execute', 'run', 'use',
        'configuration', 'setup', 'install', 'create', 'update'
    ]

    scored_chunks = []
    for chunk in chunks:
        score = 0

        # Direct keyword matching (simulating semantic similarity)
        for term in critical_terms:
            if term in chunk.lower():
                score += 2

        # Prefer chunks with examples
        if 'example' in chunk.lower() or '```' in chunk:
            score += 3

        # Prefer chunks at start (foundational)
        if chunk.startswith('#'):
            score += 2

        # Prefer complete sections
        if chunk.count('##') > 1:
            score += 1

        # Penalize very short chunks (less informative)
        if len(chunk) < 100:
            score -= 1

        scored_chunks.append((chunk, score))

    return scored_chunks

# =============================================================================
# 3. QUERY-AWARE COMPRESSION
# =============================================================================

def query_aware_compression(text, expected_queries):
    """
    Compress based on what queries are expected.

    Example queries for my core files:
    - "What are the critical rules?"
    - "What tools exist?"
    - "What commands do I run?"
    - "What is Clawd's philosophy?"
    """
    # For each expected query, identify relevant chunks
    query_relevance = {}

    for query in expected_queries:
        query_terms = query.lower().split()
        for chunk in semantic_chunking(text):
            relevance = sum(1 for term in query_terms if term in chunk.lower())
            if chunk not in query_relevance:
                query_relevance[chunk] = 0
            query_relevance[chunk] += relevance

    # Keep chunks relevant to expected queries
    relevant = [c for c, r in query_relevance.items() if r > 0]
    return '\n'.join(relevant)

# =============================================================================
# 4. TOPIC-BASED COMPRESSION
# =============================================================================

def topic_based_compression(text, target_size):
    """
    Identify topics and keep representative content per topic.

    Using LDA-like approach:
    - Identify main topics (commands, philosophy, usage, etc.)
    - Keep 1-2 key sentences per topic
    - Ensure coverage of all topics
    """
    from collections import Counter

    # Simplified topic identification
    topics = {
        'commands': [],
        'philosophy': [],
        'rules': [],
        'examples': [],
        'setup': [],
        'usage': []
    }

    # Classify chunks by topic
    for chunk in semantic_chunking(text):
        if re.search(r'(?:command|run|execute|bash|shell)', chunk, re.I):
            topics['commands'].append(chunk)
        elif re.search(r'(?:lobster|soul|essence|value|believe)', chunk, re.I):
            topics['philosophy'].append(chunk)
        elif re.search(r'(?:never|always|must|will never|choose not)', chunk, re.I):
            topics['rules'].append(chunk)
        elif re.search(r'(?:example|ex:|for instance)', chunk, re.I):
            topics['examples'].append(chunk)
        elif re.search(r'(?:install|setup|config|create|init)', chunk, re.I):
            topics['setup'].append(chunk)
        else:
            topics['usage'].append(chunk)

    # Keep representative sample per topic
    result = []
    for topic, chunks in topics.items():
        # Keep top 2 chunks per topic
        top_chunks = chunks[:2] if len(chunks) > 2 else chunks
        result.extend(top_chunks)

    return '\n'.join(result)

# =============================================================================
# 5. COHERENCE-PRESERVING COMPRESSION
# =============================================================================

def coherence_preserving_compression(text, keep_ratio=0.5):
    """
    Compress while maintaining logical flow.

    Approach:
    1. Keep first sentence of each section (context)
    2. Keep critical rules (must preserve)
    3. Keep transitions between topics
    4. Remove verbose explanations
    """
    sentences = re.split(r'(?<=[.!?])\s+', text)
    kept = []
    i = 0

    while i < len(sentences):
        sent = sentences[i]

        # Always keep section headers (if part of sentence)
        if sent.strip().startswith('##'):
            kept.append(sent)
            i += 1
            continue

        # Keep critical rules
        if re.search(r'(?:never|always|must|critical)', sent, re.I):
            kept.append(sent)
            i += 1
            continue

        # Keep topic sentences (first in paragraph)
        if i > 0 and sentences[i-1].strip().endswith('\n'):
            kept.append(sent)
            i += 1
            continue

        # Keep transitions
        if re.search(r'(?:however|therefore|additionally|firstly|secondly)', sent, re.I):
            kept.append(sent)
            i += 1
            continue

        # Skip verbose sentences with filler words
        fillers = ['actually', 'basically', 'essentially', 'in other words', 'what this means']
        if any(f in sent.lower() for f in fillers):
            i += 1
            continue

        # Decide whether to keep based on remaining quota
        current_ratio = len(' '.join(kept)) / len(text)
        if current_ratio < keep_ratio:
            # Sample sentences to fill quota
            if len(sent) < 100:  # Prefer short sentences
                kept.append(sent)

        i += 1

    return ' '.join(kept)

# =============================================================================
# 6. HYBRID APPROACH (Recommended)
# =============================================================================

def hybrid_semantic_compression(text, target_size):
    """
    Combine multiple semantic techniques.

    Pipeline:
    1. Semantic chunking (preserve structure)
    2. Topic-based classification
    3. Query-aware scoring (what will be asked?)
    4. Coherence-preserving selection
    5. Filler removal (final cleanup)
    """
    # Step 1: Semantic chunking
    chunks = semantic_chunking(text)

    # Step 2: Score by importance
    scored = semantic_importance_scoring(chunks, critical_keywords=[])

    # Step 3: Classify by topic
    topics = topic_based_compression(text, target_size)

    # Step 4: Apply coherence-preserving compression
    coherent = coherence_preserving_compression(text, keep_ratio=0.5)

    # Step 5: Final cleanup
    cleaned = remove_filler_words(coherent)

    return cleaned

# =============================================================================
# IMPLEMENTATION RECOMMENDATIONS
# =============================================================================

"""
HOW TO INCORPORATE SEMANTIC TECHNIQUES:

1. **Better Semantic Chunking**
   - Split by section headers, not line breaks
   - Merge short related paragraphs
   - Split long paragraphs at sentence boundaries
   - Result: Maintains structure while enabling granular compression

2. **Critical Item Detection**
   - Use regex patterns to find commands, paths, rules
   - Ensure these are ALWAYS preserved
   - Mark them as "non-compressible"

3. **Query Prediction**
   - What questions will be asked about this file?
   - "What commands exist?" → Keep all command blocks
   - "What are the rules?" → Keep all rule sections
   - "What's the philosophy?" → Keep SOUL.md essence

4. **Topic Coverage**
   - Ensure all topics are represented
   - Don't over-compress one topic to neglect another
   - Use balanced sampling across topics

5. **Progressive Disclosure + Semantics**
   - Level 1: Critical items only (commands, rules)
   - Level 2: Topic summaries (1-2 sentences per topic)
   - Level 3: Full details (linked files)

TOOLS NEEDED:
- Simple: Keyword matching + regex (what I used)
- Better: Sentence transformers (sentence-transformers)
- Best: Embedding models (OpenAI, Cohere, local)
"""

# Example usage for Bradley's files:

"""
AGENTS.md:
- Critical: commands, paths, tool references
- Topics: startup, tools, sub-agents, workflow
- Query-aware: "What commands exist?" "How do I start?"

SOUL.md:
- Critical: core values, boundaries
- Topics: philosophy, awareness, identity
- Query-aware: "What does Clawd believe?" "What are the rules?"

HEARTBEAT.md:
- Critical: current task status
- Topics: historical activity, ongoing projects
- Query-aware: "What am I working on?" "What's the status?"

IMPLEMENTATION:
1. Extract critical items (commands, paths, rules)
2. Classify sections by topic
3. Score sections by query relevance
4. Apply targeted compression per topic
5. Verify critical items preserved
"""
