#!/usr/bin/env python3
"""
Hacker News Daily Summary - Fetch top stories and create a GitHub gist
"""
import subprocess
import re
from datetime import datetime

def fetch_hn_stories():
    """Fetch current Hacker News stories"""
    result = subprocess.run(
        ['curl', '-s', 'https://news.ycombinator.com'],
        capture_output=True,
        text=True
    )

    html = result.stdout
    pattern = r'<a href="(https?://[^"]+)">([^<]+)</a>'
    matches = re.findall(pattern, html)

    # Filter to stories (long titles, skip HN internal links)
    stories = []
    for url, title in matches:
        if len(title) > 30 and 'ycombinator.com' not in url:
            stories.append((title, url))

    return stories[:30]  # Top 30

def create_summary(stories):
    """Create markdown summary of stories"""
    now = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")

    summary = f"""# Hacker News Daily Summary

**Generated:** {now}
**Stories:** {len(stories)}

---

## Top Stories

"""

    for i, (title, url) in enumerate(stories, 1):
        # Clean up HTML entities
        title = title.replace('&amp;', '&')
        title = title.replace('&#x27;', "'")
        title = title.replace('&quot;', '"')

        summary += f"{i}. [{title}]({url})\n"

    summary += f"""
---

*Generated by Clawd ðŸ¦ž using web scraping*
"""
    return summary

def save_summary(summary, path):
    """Save summary to file"""
    with open(path, 'w') as f:
        f.write(summary)
    print(f"Saved summary to: {path}")

def main():
    print("Fetching Hacker News stories...")
    stories = fetch_hn_stories()
    print(f"Found {len(stories)} stories\n")

    summary = create_summary(stories)

    # Save to workspace
    timestamp = datetime.utcnow().strftime("%Y%m%d-%H%M%S")
    path = f"/home/opc/clawd/hn-summary-{timestamp}.md"
    save_summary(summary, path)

    print("\nPreview:")
    print("=" * 80)
    print(summary[:800])
    print("..." if len(summary) > 800 else "")

if __name__ == "__main__":
    main()
